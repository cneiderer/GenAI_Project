{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58859582-469d-4c7d-8506-f447c0b96e89",
   "metadata": {},
   "source": [
    "# Caption Preprocessing, Tokenization, and Sequence Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7633ab-0b4a-4a10-92ee-07f58f296549",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c0c7ac2-1f03-4d07-aa5a-0c6c16e4a04e",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f80697-96a3-43b0-a8b5-eefc90689c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : genai_project\n",
      "    active env location : /opt/miniconda3/envs/genai_project\n",
      "            shell level : 2\n",
      "       user config file : /Users/shonie/.condarc\n",
      " populated config files : /opt/miniconda3/.condarc\n",
      "          conda version : 25.5.1\n",
      "    conda-build version : not installed\n",
      "         python version : 3.13.5.final.0\n",
      "                 solver : libmamba (default)\n",
      "       virtual packages : __archspec=1=m2\n",
      "                          __conda=25.5.1=0\n",
      "                          __osx=15.5=0\n",
      "                          __unix=0=0\n",
      "       base environment : /opt/miniconda3  (writable)\n",
      "      conda av data dir : /opt/miniconda3/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/osx-arm64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/osx-arm64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /opt/miniconda3/pkgs\n",
      "                          /Users/shonie/.conda/pkgs\n",
      "       envs directories : /opt/miniconda3/envs\n",
      "                          /Users/shonie/.conda/envs\n",
      "               platform : osx-arm64\n",
      "             user-agent : conda/25.5.1 requests/2.32.3 CPython/3.13.5 Darwin/24.5.0 OSX/15.5 solver/libmamba conda-libmamba-solver/25.4.0 libmambapy/2.0.5 aau/0.7.1 c/. s/. e/.\n",
      "                UID:GID : 502:20\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm environment\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781613bd-cace-4c41-8c65-4102113c3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5305e8-37f8-403d-8360-f03c41c3207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json\n",
    "from vtt.utils.config import START_TOKEN, END_TOKEN, OOV_TOKEN, MIN_WORD_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd64196-4de4-4fac-aad9-1782e0437cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize a single caption:\n",
    "    - Lowercase\n",
    "    - Remove punctuation\n",
    "    - Remove extra whitespace\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw caption string.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned caption with special tokens added.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", \"\", text)  # remove punctuation except apostrophes\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize whitespace\n",
    "    return f\"{START_TOKEN} {text} {END_TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d9b1cf-22d5-4525-a36a-fc1d5b0706ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_captions(filepath: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Load captions from a comma-separated file and clean them.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the captions CSV file (e.g., Flickr8k.token.txt).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Mapping from image filename to list of cleaned captions.\n",
    "    \"\"\"\n",
    "    captions: Dict[str, List[str]] = defaultdict(list)\n",
    "\n",
    "    with open(filepath, \"r\") as file:\n",
    "        next(file)  # Skip header\n",
    "\n",
    "        for line in file:\n",
    "            tokens = line.strip().split(\",\")\n",
    "\n",
    "            # Expect 2 tokens per line: image_name, caption\n",
    "            if len(tokens) != 2:\n",
    "                continue\n",
    "\n",
    "            image_id, caption = tokens\n",
    "            image_filename = image_id.split(\"#\")[0].strip()\n",
    "            cleaned = clean_caption(caption)\n",
    "            captions[image_filename].append(cleaned)\n",
    "\n",
    "    return dict(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b672894-3446-4217-b642-53ef70ece1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_frequencies(captions_dict: Dict[str, List[str]]) -> Counter:\n",
    "    \"\"\"\n",
    "    Count word frequencies across all captions.\n",
    "\n",
    "    Args:\n",
    "        captions_dict (Dict[str, List[str]]): Caption dictionary from `load_and_clean_captions`.\n",
    "\n",
    "    Returns:\n",
    "        Counter: Word frequency dictionary.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for captions in captions_dict.values():\n",
    "        for caption in captions:\n",
    "            counter.update(caption.split())\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d26632-b299-483d-aa27-3a05581e0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_captions_by_frequency(\n",
    "    captions_dict: Dict[str, List[str]], min_word_freq: int\n",
    ") -> Tuple[Dict[str, List[str]], Set[str]]:\n",
    "    \"\"\"\n",
    "    Replace infrequent words in captions with the OOV token.\n",
    "\n",
    "    Args:\n",
    "        captions_dict (Dict[str, List[str]]): Mapping from image filename to list of captions.\n",
    "        min_word_freq (int): Minimum number of times a word must appear to be kept.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - filtered_captions (Dict[str, List[str]]): Updated captions with rare words replaced.\n",
    "            - vocab (Set[str]): Set of retained vocabulary words.\n",
    "    \"\"\"\n",
    "    freq = count_word_frequencies(captions_dict)\n",
    "\n",
    "    vocab = {\n",
    "        word\n",
    "        for word, count in freq.items()\n",
    "        if count >= min_word_freq or word in {START_TOKEN, END_TOKEN, OOV_TOKEN}\n",
    "    }\n",
    "\n",
    "    filtered = {}\n",
    "    for img_id, captions in captions_dict.items():\n",
    "        new_captions = []\n",
    "        for caption in captions:\n",
    "            tokens = [word if word in vocab else OOV_TOKEN for word in caption.split()]\n",
    "            new_captions.append(\" \".join(tokens))\n",
    "        filtered[img_id] = new_captions\n",
    "\n",
    "    return filtered, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78de8b3e-e69e-4e52-8aed-884e61166956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(\n",
    "    filtered_captions: Dict[str, List[str]],\n",
    "    num_words: int = None,\n",
    "    oov_token: str = OOV_TOKEN,\n",
    ") -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Fit a Keras tokenizer on all captions.\n",
    "\n",
    "    Args:\n",
    "        filtered_captions (Dict[str, List[str]]): Mapping from image filename to list of cleaned captions.\n",
    "        num_words (int): Max number of words to keep based on frequency. If None, keep all.\n",
    "        oov_token (str): Token to represent out-of-vocabulary words.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: Fitted tokenizer object.\n",
    "    \"\"\"\n",
    "    all_captions = [\n",
    "        caption for captions in filtered_captions.values() for caption in captions\n",
    "    ]\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token, filters=\"\")\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482a6821-82fd-47c0-b60c-c0a0cdb5f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_to_sequences(\n",
    "    filtered_captions: Dict[str, List[str]], tokenizer: Tokenizer\n",
    ") -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Convert captions to sequences of token IDs.\n",
    "\n",
    "    Args:\n",
    "        filtered_captions (Dict[str, List[str]]): Dictionary of cleaned and filtered image captions.\n",
    "        tokenizer (Tokenizer): Fitted Keras tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[List[int]]]: Caption token sequences by image ID.\n",
    "    \"\"\"\n",
    "    seq_dict = {}\n",
    "    for img_id, captions in filtered_captions.items():\n",
    "        seq_dict[img_id] = tokenizer.texts_to_sequences(captions)\n",
    "    return seq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc09321-c534-49db-8072-f184acdaaf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_caption_sequences(\n",
    "    seq_dict: Dict[str, List[List[int]]], max_length: int\n",
    ") -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Pad or truncate token sequences to a uniform length.\n",
    "\n",
    "    Args:\n",
    "        seq_dict (Dict[str, List[List[int]]]): Tokenized caption sequences.\n",
    "        max_length (int): Max allowed sequence length.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[List[int]]]: Padded token sequences by image ID.\n",
    "    \"\"\"\n",
    "    padded_dict = {}\n",
    "    for img_id, sequences in seq_dict.items():\n",
    "        padded = pad_sequences(\n",
    "            sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    "        ).tolist()\n",
    "        padded_dict[img_id] = padded\n",
    "    return padded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31b766f0-f512-429c-b570-412a41a583cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_caption_length(\n",
    "    seq_dict: Dict[str, List[str]], quantile: float = 0.95\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Compute the maximum caption length based on a given percentile of sequence lengths.\n",
    "\n",
    "    Args:\n",
    "        seq_dict (Dict[str, List[List[int]]]): Tokenized caption sequences.\n",
    "        quantile (float): Percentile cutoff for max length (e.g., 0.95 means ignore top 5% longest captions).\n",
    "\n",
    "    Returns:\n",
    "        int: Computed maximum sequence length for padding.\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seqs in seq_dict.values() for seq in seqs]\n",
    "\n",
    "    # Use percentile cutoff to ignore extreme outliers\n",
    "    max_len = int(np.quantile(lengths, quantile))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cc4784b-648e-476b-88f8-ccf3a246d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_padded_sequences(\n",
    "    padded_dict: Dict[str, List[List[int]]], filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save padded caption sequences to a .npz file.\n",
    "\n",
    "    Args:\n",
    "        padded_dict (Dict[str, List[List[int]]]): Dict mapping image IDs to lists of padded sequences.\n",
    "        filepath (str): Output file path (should end in .npz).\n",
    "    \"\"\"\n",
    "    npz_dict = {\n",
    "        img_id: np.array(seqs, dtype=np.int32) for img_id, seqs in padded_dict.items()\n",
    "    }\n",
    "    np.savez_compressed(filepath, **npz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc05eef-3466-4f5e-987d-f744b278de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_padded_sequences(filepath: str) -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Load padded caption sequences from a .npz file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .npz file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[List[int]]]: Restored padded sequences.\n",
    "    \"\"\"\n",
    "    data = np.load(filepath, allow_pickle=True)\n",
    "    return {img_id: data[img_id].tolist() for img_id in data.files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee85b8c-c8b9-41e4-9215-518e53cd9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered captions: 31783\n",
      "# sequences with tokens: 157624\n",
      "Example sequences: [[3, 14, 22, 310, 12, 2182, 116, 195, 19, 63, 165, 27, 325, 73, 5, 6, 471, 4], [3, 14, 22, 4], [3, 14, 30, 5, 51, 262, 16, 35, 5, 2, 471, 4], [3, 2, 8, 5, 2, 28, 23, 35, 5, 2, 686, 4], [3, 14, 457, 786, 586, 15, 134, 4]]\n",
      "Max caption length: 22\n"
     ]
    }
   ],
   "source": [
    "# captions_path = \"../data/raw/flickr8k_captions.csv\"\n",
    "captions_path = \"../data/raw/flickr30k_captions.csv\"\n",
    "captions_dict = load_and_clean_captions(captions_path)\n",
    "filtered_captions, vocab = filter_captions_by_frequency(captions_dict, min_word_freq=MIN_WORD_FREQ)\n",
    "tokenizer = fit_tokenizer(filtered_captions, num_words=10000)\n",
    "seqs = captions_to_sequences(filtered_captions, tokenizer)\n",
    "max_length = compute_max_caption_length(seqs, quantile=0.95)\n",
    "padded_seqs = pad_caption_sequences(seqs, max_length=max_length)\n",
    "print(f\"# filtered captions: {len(filtered_captions)}\")\n",
    "print(f\"# sequences with tokens: {sum(len(seqs) for seqs in seqs.values())}\")\n",
    "print(\"Example sequences:\", next(iter(seqs.values()), []))\n",
    "print(\"Max caption length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9508cf2b-e645-49bd-8c33-c71d4f831d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save padded sequences\n",
    "# output_path = \"../data/processed/flickr8k_padded_caption_sequences.npz\"\n",
    "output_path = \"../data/processed/flickr30k_padded_caption_sequences.npz\"\n",
    "save_padded_sequences(padded_seqs, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2fe22ca-4ebc-4296-86dc-8ac0aa252eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption 0:\n",
      " [3, 14, 22, 310, 12, 2182, 116, 195, 19, 63, 165, 27, 325, 73, 5, 6, 471, 4, 0, 0, 0, 0]\n",
      "Caption 1:\n",
      " [3, 14, 22, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Caption 2:\n",
      " [3, 14, 30, 5, 51, 262, 16, 35, 5, 2, 471, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Caption 3:\n",
      " [3, 2, 8, 5, 2, 28, 23, 35, 5, 2, 686, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Caption 4:\n",
      " [3, 14, 457, 786, 586, 15, 134, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Load padded sequences\n",
    "loaded = load_padded_sequences(output_path)\n",
    "# Example\n",
    "# image = \"1000268201_693b08cb0e.jpg\" # flickr8k\n",
    "image = \"1000092795.jpg\"  # flickr30k\n",
    "# Print each padded caption sequence associated with the image\n",
    "for idx in range(5):\n",
    "    print(f\"Caption {idx}:\\n\", loaded[image][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8143030-636c-43ce-a50e-aadf6582d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need functions to save and load trained tokenizer\n",
    "# save_tokenizer(filepath)\n",
    "# load_tokenizer(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99da88f-b363-4967-a154-d592850a9fc4",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b564e-5601-48e4-8142-79a3c152411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer: Tokenizer, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Save tokenizer to a pickle file.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): Fitted Keras tokenizer.\n",
    "        filepath (str): Output path for saving.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289ef98-f646-4042-90f9-09db78a17ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(filepath: str) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Load a tokenizer from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the tokenizer file.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: Loaded tokenizer.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd79b6-6adb-4ce6-8e7b-c2d803dd0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer_json(tokenizer, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a Keras tokenizer to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): Trained tokenizer.\n",
    "        filepath (str): Destination JSON path.\n",
    "    \"\"\"\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c2324-f33b-482c-94fb-340ceb642519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_json(filepath: str):\n",
    "    \"\"\"\n",
    "    Load a Keras tokenizer from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to saved JSON tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: Reconstructed tokenizer.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokenizer_json = json.load(f)\n",
    "    return tokenizer_from_json(tokenizer_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
