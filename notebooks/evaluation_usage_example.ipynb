{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421a1bbc-0276-43cc-ae19-78d5e11079ce",
   "metadata": {},
   "source": [
    "# Evaluation Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f265f2-0d47-4767-a530-8c77611adc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f626dd09-8c10-40ee-9249-e2b0e84b9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vtt.data.caption_preprocessing import load_tokenizer, load_and_clean_captions\n",
    "from vtt.data.data_loader import load_split_datasets\n",
    "from vtt.models.decoder import build_decoder_model\n",
    "from vtt.evaluation.evaluate import evaluate_model\n",
    "from vtt.evaluation.evaluate import evaluate_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb13d6",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b62e76-5579-4843-ab85-237bb0368c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/anaconda3/envs/northeastern/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.7857\n",
      "BLEU-2: 0.7182\n",
      "BLEU-3: 0.5169\n",
      "BLEU-4: 0.4293\n",
      "METEOR: 0.7936\n",
      "BERTScore_P: 0.9723\n",
      "BERTScore_R: 0.9735\n",
      "BERTScore_F1: 0.9729\n"
     ]
    }
   ],
   "source": [
    "ground_truths = {\n",
    "    \"image1.jpg\": [\"a man riding a bike\", \"a person on a bicycle in motion\"],\n",
    "    \"image2.jpg\": [\"a cat sitting on a couch\", \"a feline on furniture\"],\n",
    "}\n",
    "\n",
    "generated = {\n",
    "    \"image1.jpg\": \"a man riding a bicycle\",\n",
    "    \"image2.jpg\": \"a cat is lying on a sofa\",\n",
    "}\n",
    "\n",
    "scores = evaluate_captions(ground_truths, generated)\n",
    "for metric, value in scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641043fd",
   "metadata": {},
   "source": [
    "## Caption Prediction on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875988ed",
   "metadata": {},
   "source": [
    "### Fetch the Test dataset fro evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6ea4f0-8616-4508-a986-9a39015300ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tokenizer loaded from JSON file: ../data/processed/flickr8k_tokenizer.json\n",
      "\n",
      "--- Dataset Split Sizes (number of individual samples) ---\n",
      "Total samples loaded: 38008\n",
      "Train samples: 28507\n",
      "Validation samples: 5701\n",
      "Test samples: 3800\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"flickr8k\"\n",
    "features_path = f\"../data/processed/{dataset_name}_features.npz\"\n",
    "captions_path = f\"../data/processed/{dataset_name}_padded_caption_sequences.npz\"\n",
    "tokenizer_path = f\"../data/processed/{dataset_name}_tokenizer.json\"\n",
    "\n",
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "features = np.load(features_path)\n",
    "\n",
    "train_ds, val_ds, test_ds = load_split_datasets(\n",
    "    features_path=features_path,\n",
    "    captions_path=captions_path,\n",
    "    batch_size=64,\n",
    "    val_split=0.15,\n",
    "    test_split=0.10,\n",
    "    shuffle=True,\n",
    "    buffer_size=1000,\n",
    "    seed=42,\n",
    "    cache=True,\n",
    "    return_numpy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653324d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 14:14:59.103519: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get max caption length from dataset\n",
    "for (image_tensor, input_caption, _), _ in train_ds.take(1):\n",
    "    max_caption_len = input_caption.shape[1]\n",
    "# Get vocab size\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2523908",
   "metadata": {},
   "source": [
    "### Load the Saved Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11b2761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = build_decoder_model(vocab_size=vocab_size,\n",
    "                            max_caption_len=max_caption_len)\n",
    "\n",
    "checkpoint_path = \"../models/flickr8k_decoder_weights.weights.h5\"\n",
    "model.load_weights(checkpoint_path)\n",
    "print(\"Loaded pretrained weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba7743",
   "metadata": {},
   "source": [
    "### Evaluate Scores for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901c6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions from Dataset: 100%|██████████| 60/60 [24:19<00:00, 19.78s/it]2025-07-18 13:32:50.523074: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Generating Captions from Dataset: 100%|██████████| 60/60 [24:19<00:00, 24.32s/it]\n",
      "/opt/miniconda3/envs/genai_project/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "W0718 13:32:57.799000 12095 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "references_dict = load_and_clean_captions(f\"../data/raw/{dataset_name}_captions.csv\")\n",
    "\n",
    "scores = evaluate_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    features=features,\n",
    "    test_dataset=test_ds,\n",
    "    references_dict=references_dict,\n",
    "    max_len=max_caption_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490d98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores:\n",
      "BLEU-1: 0.4705\n",
      "BLEU-2: 0.2836\n",
      "BLEU-3: 0.1710\n",
      "BLEU-4: 0.1126\n",
      "METEOR: 0.2661\n",
      "BERTScore_P: 0.8854\n",
      "BERTScore_R: 0.8552\n",
      "BERTScore_F1: 0.8699\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation Scores:\")\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
