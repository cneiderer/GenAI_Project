{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6716c7b-1103-4335-8123-ab4b5ce14b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:08:29.529709: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752192509.550248 4181949 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752192509.556592 4181949 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752192509.572332 4181949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752192509.572349 4181949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752192509.572352 4181949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752192509.572353 4181949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 20:08:29.579595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import pickle   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c37881-5b3a-43b9-be45-5682ed2751b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7193fb38-a1fe-430e-91f0-8b04dcf199ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "captions_path = \"./padded_sequences.npz\"        \n",
    "captions = np.load(captions_path, allow_pickle=True)\n",
    "caption_data = {img_id: captions[img_id] for img_id in captions.files}\n",
    "\n",
    "image_features_path = \"./flickr8k_features.npz\" \n",
    "features_npz = np.load(image_features_path, allow_pickle=True)\n",
    "image_features = {k: features_npz[k] for k in features_npz.files}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c52601e-ca7a-4745-8a60-a91ecd652a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./flickr8k_tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13471d-81fd-49bb-a7e0-bb3d15ebcaab",
   "metadata": {},
   "source": [
    "captions_path = \"/Users/desireereid/Downloads/padded_sequences.npz\"\n",
    "captions = np.load(captions_path, allow_pickle=True)\n",
    "caption_data = {img_id: captions[img_id] for img_id in captions.files} -- if you're running locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce4436a-c536-49d3-8a3b-ec74e60c6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "UNITS = 512\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1   \n",
    "MAX_LEN = max(len(seq) for seqs in caption_data.values() for seq in seqs)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a5ef4-b990-4cfb-bf58-9ea8e3eb44c4",
   "metadata": {},
   "source": [
    "**Note:** The values for `VOCAB_SIZE` and `MAX_LEN` are placeholders for now.  \n",
    "- `VOCAB_SIZE = 5000` is a rough estimate until the tokenizer is available.  \n",
    "- `MAX_LEN = 34` is a reasonable max caption length based on the dataset.  \n",
    "Once the tokenizer is loaded, update:\n",
    "- `VOCAB_SIZE = len(tokenizer.word_index) + 1`\n",
    "- `MAX_LEN` = actual max length of the padded captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca74779-747b-47f8-b594-35d231823511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding  = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.image_proj = layers.Dense(units, activation='relu')\n",
    "        self.lstm       = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.fc         = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        img_feat, captions = inputs          \n",
    "        img_proj = self.image_proj(img_feat) \n",
    "        img_proj = tf.expand_dims(img_proj, 1)\n",
    "\n",
    "        cap_emb  = self.embedding(captions)  \n",
    "        lstm_in  = tf.concat([img_proj, cap_emb], axis=1)\n",
    "\n",
    "        lstm_out, _, _ = self.lstm(lstm_in)\n",
    "        return self.fc(lstm_out)             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dcd796e-c874-450d-bc02-2467aa5343ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:13:18.500592: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-07-10 20:13:18.501486: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2025-07-10 20:13:18.501499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2025-07-10 20:13:18.501508: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-07-10 20:13:18.501513: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: c2190\n",
      "2025-07-10 20:13:18.501517: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: c2190\n",
      "2025-07-10 20:13:18.501620: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 545.23.8\n",
      "2025-07-10 20:13:18.501644: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 545.23.8\n",
      "2025-07-10 20:13:18.501648: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 545.23.8\n"
     ]
    }
   ],
   "source": [
    "model = ImageCaptioningModel(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    units=UNITS,\n",
    "    vocab_size=VOCAB_SIZE\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93c2f7b-ed03-40e2-a707-6cf02135f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 20, 8481)\n"
     ]
    }
   ],
   "source": [
    "sample_img     = list(image_features.values())[0].reshape(1, -1)       \n",
    "sample_caption = np.array([caption_data[list(caption_data.keys())[0]][0]]) \n",
    "\n",
    "model = ImageCaptioningModel(embed_dim=EMBED_DIM,\n",
    "                             units=UNITS,\n",
    "                             vocab_size=VOCAB_SIZE)\n",
    "\n",
    "output = model((sample_img, sample_caption))  \n",
    "print(\"Output shape:\", output.shape)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a63d2-301b-4099-a212-303fa97469ef",
   "metadata": {},
   "source": [
    " #### Build a tf.data.Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036643ec-6013-44f0-9b8e-af6cc500a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def make_dataset(img_feat_dict, cap_dict, batch_size=32, shuffle=True):\n",
    "    img_ids = list(img_feat_dict.keys())               \n",
    "    img_feats = [img_feat_dict[i] for i in img_ids]      \n",
    "    caps      = [cap_dict[i]      for i in img_ids]      \n",
    "\n",
    "    flat_feats = []\n",
    "    flat_caps  = []\n",
    "    for f, five_caps in zip(img_feats, caps):\n",
    "        for c in five_caps:\n",
    "            flat_feats.append(f)\n",
    "            flat_caps.append(c)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((flat_feats, flat_caps))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(flat_feats))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(image_features, caption_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275fcd6-20ef-4d63-8857-0554199a80be",
   "metadata": {},
   "source": [
    "#### Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9b2f08-f29c-408a-a3a3-0131f86d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn   = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Ignore padding-token positions (0) when computing loss.\n",
    "    \"\"\"\n",
    "    mask  = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss  = loss_fn(y_true, y_pred) * mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061791cb-7367-41f8-9afb-92b84c315395",
   "metadata": {},
   "source": [
    "#### One Traning Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2018e56-1ac9-47cf-a899-8533ee5a4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_batch, cap_batch):\n",
    "    \"\"\"\n",
    "    img_batch : (B, 2048)\n",
    "    cap_batch : (B, MAX_LEN)\n",
    "    \n",
    "    We feed caption[:-1] as input and predict caption[1:].\n",
    "    \"\"\"\n",
    "    inp  = cap_batch[:, :-1]   \n",
    "    targ = cap_batch[:, 1:]    \n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model((img_batch, inp))     \n",
    "\n",
    "        logits = logits[:, 1:, :]           \n",
    "\n",
    "        loss   = masked_loss(targ, logits)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5789ea-f008-44a3-b5a7-4f3c25f52465",
   "metadata": {},
   "source": [
    "#### Small Traning Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733235cd-564e-4f7d-94ca-aa5bbe65e09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:20:49.465374: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 – avg loss: 9.0476\n",
      "Epoch 2 – avg loss: 8.9786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:20:50.182777: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2          \n",
    "STEPS  = 100        \n",
    "for epoch in range(EPOCHS):\n",
    "    total, steps = 0.0, 0\n",
    "\n",
    "    for img_b, cap_b in train_ds.take(STEPS):\n",
    "        batch_loss = train_step(img_b, cap_b)   \n",
    "        total += float(batch_loss)              \n",
    "        steps += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1} – avg loss: {total/steps:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35303bb7-1be6-48c9-a385-92d47615033d",
   "metadata": {},
   "source": [
    "#### Full Training Loop w/patience + checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb824524-e614-4beb-98c7-a12ced95f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, datetime, pickle, numpy as np, tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bf1a689-dde1-4516-8f54-ad5419d1f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature vectors\n",
    "feat_path = pathlib.Path(\"./flickr8k_features.npz\")\n",
    "features_npz = np.load(feat_path, allow_pickle=True)\n",
    "image_features = {k: features_npz[k] for k in features_npz.files}\n",
    "\n",
    "# padded caption sequences\n",
    "cap_path = pathlib.Path(\"./padded_sequences.npz\")\n",
    "caps_npz = np.load(cap_path, allow_pickle=True)\n",
    "caption_data = {k: caps_npz[k] for k in caps_npz.files}\n",
    "\n",
    "# tokenizer \n",
    "with open(\"./flickr8k_tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1          \n",
    "MAX_LEN    = next(iter(caption_data.values()))[0].shape[-1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95eeb5e1-5ea2-4d85-8b55-32f18f7e826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images   |   max-len = 19\n"
     ]
    }
   ],
   "source": [
    "print(len(image_features), \"images   |   max-len =\", MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5aa2530-115a-4e44-b144-35452b938fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example batch shapes: (4, 2048) (4, 18) (4, 18)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH = 4          # set 32-64 once we have the full dataset\n",
    "BUFFER = 8_000\n",
    "\n",
    "def make_dataset(img_dict, cap_dict):\n",
    "    \"\"\"Yields (img_vec, caption_in)  → caption_out.\"\"\"\n",
    "    def gen():\n",
    "        for img_id, feats in img_dict.items():\n",
    "            for cap in cap_dict[img_id]:\n",
    "                yield feats.astype(\"float32\"), np.array(cap, dtype=\"int32\")\n",
    "\n",
    "    raw_ds = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(2048,),    dtype=tf.float32),  \n",
    "            tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32)    \n",
    "        )\n",
    "    )\n",
    "\n",
    "    def prep(feats, full_cap):\n",
    "        return (feats, full_cap[:-1]), full_cap[1:]\n",
    "\n",
    "    return (raw_ds\n",
    "              .shuffle(BUFFER)\n",
    "              .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .batch(BATCH, drop_remainder=True)        \n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds = make_dataset(image_features, caption_data)\n",
    "print(\"example batch shapes:\",\n",
    "      next(iter(train_ds.take(1)))[0][0].shape,   \n",
    "      next(iter(train_ds.take(1)))[0][1].shape,  \n",
    "      next(iter(train_ds.take(1)))[1].shape)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bea754b1-a09e-4e3e-9ab1-3e5a93939c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=\"none\")\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true : (B, T)           – 18 tokens\n",
    "    y_pred : (B, T+1, V)      – 19 logits (image step + T tokens)\n",
    "    We remove the first step so that time-dims match.\n",
    "    \"\"\"\n",
    "    y_pred = y_pred[:, 1:, :]          # <--- crop image timestep\n",
    "    mask   = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "\n",
    "    loss   = loss_fn(y_true, y_pred) * mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6912dd8f-27e8-452a-822d-c75583ffbc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "     11/Unknown \u001b[1m4s\u001b[0m 163ms/step - loss: 8.5292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reid.des/.local/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - loss: 8.4009\n",
      "Epoch 2/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 197ms/step - loss: 4.7350\n",
      "Epoch 3/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 4.3368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:43:31.153379: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 201ms/step - loss: 4.3091\n",
      "Epoch 4/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 191ms/step - loss: 4.2096\n",
      "Epoch 5/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - loss: 4.1474\n",
      "Epoch 6/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step - loss: 4.0147\n",
      "Epoch 7/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - loss: 3.8208\n",
      "Epoch 8/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - loss: 3.7162\n",
      "Epoch 9/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - loss: 3.6558\n",
      "Epoch 10/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - loss: 3.5262\n",
      "Epoch 11/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 3.4196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:43:48.994611: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 193ms/step - loss: 3.3914\n",
      "Epoch 12/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - loss: 3.2273\n",
      "Epoch 13/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step - loss: 3.1843\n",
      "Epoch 14/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - loss: 3.0316\n",
      "Epoch 15/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 194ms/step - loss: 2.8535\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=masked_loss)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[ckpt_cb, early_cb, tb_cb]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84993fc6-c4eb-4781-9796-7cdeb39bfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "np.savez('training_meta.npz', max_len=MAX_LEN, vocab_size=VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3909486-3d85-49bb-bc03-24aa29eeec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded ✅\n"
     ]
    }
   ],
   "source": [
    "model = ImageCaptioningModel(EMBED_DIM, UNITS, VOCAB_SIZE)\n",
    "\n",
    "\n",
    "dummy_img  = tf.zeros((1, 2048), dtype=tf.float32)   # (B, 2048)\n",
    "dummy_cap  = tf.zeros((1, MAX_LEN), dtype=tf.int32)  # (B, MAX_LEN)\n",
    "_ = model((dummy_img, dummy_cap))                    # build graph\n",
    "\n",
    "\n",
    "model.load_weights(\"checkpoints/best.weights.h5\")\n",
    "print(\"Weights loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10306675-b878-4dec-bb8d-e1a2b6e63f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reid.des/.local/lib/python3.12/site-packages/keras/src/layers/layer.py:396: UserWarning: `build()` was called on layer 'image_captioning_model_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ImageCaptioningModel(EMBED_DIM, UNITS, VOCAB_SIZE)\n",
    "\n",
    "model.build(input_shape=[(None, 2048), (None, MAX_LEN)])\n",
    "\n",
    "model.load_weights(\"checkpoints/best.weights.h5\")\n",
    "print(\"Weights loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "005cfbc4-aad5-4578-91f0-7ae426b6c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: (1, 20, 8481)\n"
     ]
    }
   ],
   "source": [
    "sample_img = list(image_features.values())[0].reshape(1, -1)\n",
    "start_seq  = [tokenizer.word_index.get(\"<start>\", 1)]\n",
    "caption_in = tf.constant(start_seq + [0]*(MAX_LEN-1))[None, :]\n",
    "\n",
    "logits = model((sample_img, caption_in))\n",
    "print(\"Output logits shape:\", logits.shape)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d85bd3a-80dd-4554-9fba-6ddb6f87d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_ID = 3  |  END_ID = 4\n",
      "\n",
      "Generated caption:\n",
      " bluestriped bluestriped bluestriped bluestriped bluestriped intensely intensely intensely sidecar accross intensely filed filed eyese eyese eyese labeled showering\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "one_img_id      = next(iter(caption_data))           # pick any key\n",
    "one_caption_seq = caption_data[one_img_id][0]        # first caption list\n",
    "START_ID        = int(one_caption_seq[0])            # first token in every seq\n",
    "END_ID          = int(one_caption_seq[-1])           # last token (padding-aware)\n",
    "\n",
    "print(f\"START_ID = {START_ID}  |  END_ID = {END_ID}\")\n",
    "\n",
    "\n",
    "def greedy_caption(model, img_vec, tokenizer,\n",
    "                   start_id=START_ID, end_id=END_ID,\n",
    "                   max_len=one_caption_seq.shape[-1]):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        img_vec : (1, 2048) float32 numpy / tensor\n",
    "        tokenizer : fitted Keras tokenizer\n",
    "    Returns\n",
    "        string  – generated caption (without <start>/<end>)\n",
    "    \"\"\"\n",
    "    caption_ids = [start_id]                         \n",
    "    for _ in range(max_len - 1):\n",
    "        inp = tf.constant([caption_ids + [0]*(max_len - len(caption_ids))],\n",
    "                          dtype=tf.int32)\n",
    "        logits = model((img_vec, inp))[:, 1:, :]     \n",
    "        next_id = int(tf.argmax(logits[0, len(caption_ids)-1]).numpy())\n",
    "\n",
    "        if next_id in (end_id, 0):                   \n",
    "            break\n",
    "        caption_ids.append(next_id)\n",
    "\n",
    "    inv_vocab = {i:w for w,i in tokenizer.word_index.items()}\n",
    "    words = [inv_vocab.get(i, \"<unk>\") for i in caption_ids[1:]]  \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "sample_img_vec = list(image_features.values())[0].reshape(1, -1).astype(\"float32\")\n",
    "\n",
    "generated = greedy_caption(model, sample_img_vec, tokenizer)\n",
    "print(\"\\nGenerated caption:\\n\", generated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
