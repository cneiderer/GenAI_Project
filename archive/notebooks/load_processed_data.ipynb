{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376aa6c4-f272-4f6a-ba43-d88db60788f1",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb68e3-c118-4b4b-b304-e3b39671da9e",
   "metadata": {},
   "source": [
    "Loads and combines the image features with the padded sequences into a Tensorflow dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36407ef5-8514-44c0-b7ee-c167d1f00ff1",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b15bc1-8820-4483-b8bc-87d20d45d005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : northeastern\n",
      "    active env location : /home/curtis/anaconda3/envs/northeastern\n",
      "            shell level : 2\n",
      "       user config file : /home/curtis/.condarc\n",
      " populated config files : /home/curtis/anaconda3/.condarc\n",
      "          conda version : 24.9.2\n",
      "    conda-build version : 24.9.0\n",
      "         python version : 3.12.7.final.0\n",
      "                 solver : libmamba (default)\n",
      "       virtual packages : __archspec=1=skylake\n",
      "                          __conda=24.9.2=0\n",
      "                          __glibc=2.39=0\n",
      "                          __linux=6.6.87.2=0\n",
      "                          __unix=0=0\n",
      "       base environment : /home/curtis/anaconda3  (writable)\n",
      "      conda av data dir : /home/curtis/anaconda3/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /home/curtis/anaconda3/pkgs\n",
      "                          /home/curtis/.conda/pkgs\n",
      "       envs directories : /home/curtis/anaconda3/envs\n",
      "                          /home/curtis/.conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/24.9.2 requests/2.32.3 CPython/3.12.7 Linux/6.6.87.2-microsoft-standard-WSL2 ubuntu/24.04.1 glibc/2.39 solver/libmamba conda-libmamba-solver/24.9.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.\n",
      "                UID:GID : 1000:1000\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm environment\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24809dd0-b2c3-44ac-b6ae-57c9d9c7284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fae0f6-1fb5-46a6-b9b2-3111949334ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:24:35.206735: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-10 18:24:35.218185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752186275.233925  911290 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752186275.242826  911290 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752186275.264297  911290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752186275.264322  911290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752186275.264323  911290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752186275.264324  911290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 18:24:35.269654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\n",
    "from vtt.utils import detect_and_set_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82882ea0-5342-4f8d-aae7-6339f1c61445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU devices found despite TensorFlow being built with CUDA. Using CPU.\n",
      "TensorFlow is configured to use: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:24:38.580987: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Detect and set up GPU or use CPU\n",
    "device_used = detect_and_set_device()\n",
    "print(f\"TensorFlow is configured to use: {device_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06919c57-fb44-4e71-8537-ecd2aac5bdda",
   "metadata": {},
   "source": [
    "## Combine Features and Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3652bbd-781b-4146-8001-5f052819ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_features_and_sequences(\n",
    "    features_path: str,\n",
    "    captions_path: str,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    buffer_size: int = 1000,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Load image features and padded caption sequences and return a batched tf.data.Dataset.\n",
    "\n",
    "    Args:\n",
    "        features_path (str): Path to .npz file with image features.\n",
    "        captions_path (str): Path to .npz file with padded caption sequences.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "        buffer_size (int): Buffer size for shuffling.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset of (image_feature, caption_sequence) pairs.\n",
    "    \"\"\"\n",
    "    features_npz = np.load(features_path)\n",
    "    captions_npz = np.load(captions_path, allow_pickle=True)\n",
    "\n",
    "    image_features = []\n",
    "    caption_sequences = []\n",
    "\n",
    "    for img_id in captions_npz.files:\n",
    "        if img_id not in features_npz:\n",
    "            continue  # Skip if image feature is missing\n",
    "\n",
    "        feature = features_npz[img_id]\n",
    "        captions = captions_npz[img_id]\n",
    "\n",
    "        for caption in captions:\n",
    "            image_features.append(feature)\n",
    "            caption_sequences.append(caption)\n",
    "\n",
    "    # Convert to tensors\n",
    "    image_features = tf.convert_to_tensor(image_features, dtype=tf.float32)\n",
    "    caption_sequences = tf.convert_to_tensor(caption_sequences, dtype=tf.int32)\n",
    "\n",
    "    # Build dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_features, caption_sequences))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99497719-0ace-45b0-babe-3ae9764ca16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = \"../data/processed/flickr8k_features.npz\"\n",
    "padded_sequences = \"../data/processed/flickr8k_padded_caption_sequences.npz\"\n",
    "dataset = load_features_and_sequences(image_features, padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e92657d-30d2-499b-9a47-c2d5da502eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape: (2048,)\n",
      "Padded caption sequence: [   3    2   43    5    2   92  172    8  115   52    2  397   13  378\n",
      "    5   29 5005  690    4]\n",
      "---\n",
      "Image features shape: (2048,)\n",
      "Padded caption sequence: [  3   2  19 304  63   2 189 116   4   0   0   0   0   0   0   0   0   0\n",
      "   0]\n",
      "---\n",
      "Image features shape: (2048,)\n",
      "Padded caption sequence: [   3    2   38   19  115   63    2  189 2330    4    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "---\n",
      "Image features shape: (2048,)\n",
      "Padded caption sequence: [   3    2   38   19  115    6  378   21   62 2330    4    0    0    0\n",
      "    0    0    0    0    0]\n",
      "---\n",
      "Image features shape: (2048,)\n",
      "Padded caption sequence: [   3    2   38   19    5    2   92  172  304   63    2  189 3267    4\n",
      "    0    0    0    0    0]\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:28:45.341219: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for img_feat, seq in dataset.take(5):\n",
    "    print(\"Image features shape:\", img_feat.shape)\n",
    "    print(\"Padded caption sequence:\", seq.numpy())\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dceb0df-a68e-40b7-9143-33168393f282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1d4df-aa4b-4b41-a560-a8b731af7447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4991acb-183a-4bf4-a6e7-120109c97aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "048ec871-9299-41b2-a949-ac463535b200",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8785814-bc06-4f5d-8b07-881b7213efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_caption_model(\n",
    "    vocab_size: int,\n",
    "    max_caption_len: int,\n",
    "    embedding_dim: int = 256,\n",
    "    lstm_units: int = 512,\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Build an image captioning model using LSTM and pretrained image features.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the tokenizer vocabulary.\n",
    "        max_caption_len (int): Max length of caption sequences.\n",
    "        embedding_dim (int): Size of word embeddings.\n",
    "        lstm_units (int): Number of LSTM units.\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    # Image feature input (2048-dim)\n",
    "    img_input = Input(shape=(2048,), name=\"image_features\")\n",
    "    img_dense = Dense(embedding_dim, activation=\"relu\")(img_input)\n",
    "    img_dropout = Dropout(0.5)(img_dense)\n",
    "\n",
    "    # Caption sequence input\n",
    "    caption_input = Input(shape=(max_caption_len,), name=\"caption_sequence\")\n",
    "    caption_embed = Embedding(vocab_size, embedding_dim, mask_zero=True)(caption_input)\n",
    "    caption_dropout = Dropout(0.5)(caption_embed)\n",
    "\n",
    "    # Combine image + text\n",
    "    merged = Concatenate()([tf.expand_dims(img_dropout, 1), caption_dropout])\n",
    "    lstm_out = LSTM(lstm_units)(merged)\n",
    "    output = Dense(vocab_size, activation=\"softmax\")(lstm_out)\n",
    "\n",
    "    model = Model(inputs=[img_input, caption_input], outputs=output)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274b33b-13ea-4091-aa0c-b937922556f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_dataset(dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Shift target tokens for teacher forcing.\n",
    "\n",
    "    Input:\n",
    "        (image_features, [start, w1, w2, ..., wn])\n",
    "    Output:\n",
    "        inputs = (image_features, [start, w1, ..., wn-1])\n",
    "        target = wn\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: ((img, input_seq), target_seq) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def map_fn(img, caption):\n",
    "        input_seq = caption[:-1]\n",
    "        target = caption[1:]\n",
    "        return (img, input_seq), target\n",
    "\n",
    "    return dataset.map(map_fn).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4510c51-bbfd-4e28-886a-887aea581c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and summarize the model\n",
    "model = build_caption_model(tokenizer.num_words, padded_sequences.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# Prepare dataset\n",
    "train_dataset = load_features_and_sequences(\n",
    "    features_path=\".../flickr8k_features_combined.npz\",\n",
    "    captions_path=\".../flickr8k_sequences.npz\",\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataset = prepare_training_dataset(train_dataset)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e01a1-62ed-4796-ad16-4f923446eb16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "northeastern",
   "language": "python",
   "name": "northeastern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
